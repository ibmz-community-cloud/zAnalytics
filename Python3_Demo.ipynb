{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Retention Demo Using Python\n",
    "In this demo, we will show Anaconda functionality accessing enterprise data from VSAM and DB2. The data stored in VSAM consists of 6,001 rows of customer information.  The data stored in DB2 consists of 20,000 rows of transaction data. The data is transformed and joined in a Pandas DataFrame, which is used to perform exploratory analyses. A random forest algorithm is then used to predict customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USERNAME=\"???????\"\n",
    "PASSWORD=\"????????\"\n",
    "MDSS_SSID=\"AZK1\"\n",
    "DB2_SSID=\"DBBG\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Mainframe Data Connections\n",
    "This step will set up the VSAM and DB2 connections to access the data and load them into Pandas DataFrames.  The dsdbc module is delivered with the z/OS IzODA Anaconda distribution. It enables Python applications to access the z/OS IzODA Mainframe Data Service. The Data Service component (MDS) provides optimized, virtualized, and parallelized access to both IBM Z data sources and other off-platform data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cp1047_to_utf8(list):\n",
    "    list_out = []\n",
    "    for e in list:\n",
    "        x = ()\n",
    "        for i in e:\n",
    "            if isinstance(i, (str,)):\n",
    "                s = i.encode('utf16').decode('cp1047').encode('utf8').decode('utf16')[2:]\n",
    "                x = x + (s,)\n",
    "            else:\n",
    "                x = x + (i,)\n",
    "        list_out.append(x)\n",
    "    return list_out\n",
    "\n",
    "def load_data_from_mds(vtable_name, user, password, mds_id=MDSS_SSID):\n",
    "    import dsdbc\n",
    "    conn =dsdbc.connect(SSID=mds_id, user=user, password=password)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM \" + vtable_name)\n",
    "    rows = cursor.fetchall()\n",
    "    label = []\n",
    "    for col in cursor.description: label.append(col[0].lower())\n",
    "    conn.close()\n",
    "    return pd.DataFrame(rows, columns=label)\n",
    "\n",
    "def load_data_from_db2(table_name, user, password, mds_id=MDSS_SSID,  db2_id=DB2_SSID):\n",
    "    import dsdbc\n",
    "    conn =dsdbc.connect(SSID=mds_id, user=user, password=password, dsid=db2_id)\n",
    "    cursor = conn.cursor()\n",
    "    sql = \"SELECT * FROM \" + table_name\n",
    "    #print(sql)\n",
    "    cursor.execute(sql)\n",
    "    rows = cp1047_to_utf8(cursor.fetchall())\n",
    "    label = []\n",
    "    for col in cursor.description: label.append(col[0].lower())\n",
    "    conn.close()\n",
    "    return pd.DataFrame(rows, columns=label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Credit card transactions***\n",
    "\n",
    "Load credit card transactions into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txn_df = load_data_from_db2(table_name='SPARKDB.SPPAYTB1', user=USERNAME, password=PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txn_df['acaureq_aureq_tx_dt_ttlamt'] = pd.to_numeric(txn_df['acaureq_aureq_tx_dt_ttlamt'])\n",
    "txn_df['cont_id'] = txn_df['cont_id'].astype('int64')\n",
    "txn_df['acaureq_hdr_credtt'] = pd.to_datetime(txn_df['acaureq_hdr_credtt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txn_df['date'] = txn_df['acaureq_hdr_credtt'].apply(lambda x: x.date())\n",
    "txn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Client Data***\n",
    "\n",
    "Load client data into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client_df = load_data_from_mds(vtable_name='VSAM_CLIENT', user=USERNAME, password=PASSWORD)\n",
    "client_df = client_df.set_index(\"cont_id\")\n",
    "client_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate statistics\n",
    "Calculate a few aggregate statistics based on credit transactions and join the results to the client data DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Total transactions per customer\n",
    "total_txns_df = txn_df.groupby('cont_id').size().rename(\"total_txns\").to_frame()\n",
    "client_df = total_txns_df.join(client_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Total transaction amounts per customer\n",
    "total_txn_amount_df = txn_df.groupby('cont_id')['acaureq_aureq_tx_dt_ttlamt'].sum().rename(\"total_txn_amount\").to_frame()\n",
    "client_df = client_df.join(total_txn_amount_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Average transaction amounts per customer\n",
    "avg_txn_amount_df = txn_df.groupby('cont_id')['acaureq_aureq_tx_dt_ttlamt'].mean().rename(\"avg_txn_amount\").to_frame()\n",
    "client_df = client_df.join(avg_txn_amount_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Average daily transactions per customer\n",
    "daily_txns = txn_df.groupby(['date', 'cont_id']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Missing transactions on a particular day means customer had none.\n",
    "# These days should be included in the average as 0 transaction days.\n",
    "avg_daily_txns_df = daily_txns.unstack().fillna(0).mean().rename(\"avg_daily_txns\").to_frame()\n",
    "client_df = client_df.join(avg_daily_txns_df)\n",
    "client_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analyses\n",
    "We begin our exploration of the data set by creating a scatterplot of annual_income vs. age_years and their associated histograms. Matplotlib and Seaborn are two common plotting libraries used in Python.  These plotting libraries are useful in creating custom visualizations to help gain insights from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
	"warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jointplot(x, y, data, **kwargs):\n",
    "    size = kwargs.pop('size', 10)\n",
    "    alpha = kwargs.pop('alpha', 0.3)\n",
    "    return sns.jointplot(x=x, y=y, data=data, \n",
    "                         alpha=alpha,\n",
    "                         size=size,\n",
    "                         **kwargs)\n",
    "\n",
    "# for widget\n",
    "def w_jointplot(x, y):\n",
    "    g = jointplot(x, y, filter_outliers(client_df, by_col=y))\n",
    "    plt.close()\n",
    "    return g.fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "churn_labels = ['Did not churn', 'Did churn']\n",
    "\n",
    "def filter_outliers(d, by_col=None):\n",
    "    if isinstance(d, pd.\n",
    "                  Series):\n",
    "        return d[((d-d.mean()).abs()<=3*d.std())]\n",
    "    elif isinstance(d, pd.DataFrame):\n",
    "        if not by_col:\n",
    "            raise ValueError('by_col is required for DataFrame')\n",
    "        return d[np.abs(d[by_col]-d[by_col].mean())<=(3*d[by_col].std())] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = jointplot('age_years', 'annual_income', filter_outliers(client_df, by_col='annual_income'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "Next, we compute the correlation coefficients between each variable and create a color-coded correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr = client_df.corr()\n",
    "\n",
    "# only show lower triangle\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12,12))\n",
    "ax = sns.heatmap(corr, mask=mask, square=True, annot=True, fmt='.2f',\n",
    "                 cbar=True,\n",
    "                 ax=ax)\n",
    "title = ax.set_title('Correlations', size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn\n",
    "Here we plot the distributions of clients who did and did not churn. The green histogram shows the number of clients who did churn. The blue histogram shows the number of clients who did not churn.  The line graphs show the density functions for each case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_churn_by(df, col, **kwargs):\n",
    "    f, ax = plt.subplots(figsize=(12,10), sharex=True)\n",
    "    kde = kwargs.get('kde', False)\n",
    "    hist = kwargs.get('hist', False)\n",
    "    for churn in df.churn.unique():\n",
    "        sns.distplot(df[df.churn == churn][col], \n",
    "                     label=churn_labels[churn], \n",
    "                     kde_kws={'shade': (kde and not hist)},\n",
    "                     ax=ax, \n",
    "                     **kwargs)\n",
    "\n",
    "    ax.set_title('Client Churn by {}'.format(col))\n",
    "    label = ax.set_xlabel('{}'.format(col))\n",
    "    return f, ax\n",
    "\n",
    "def w_plot_churn_by(column, hist=True, kde=False, norm_hist=False):\n",
    "    df = filter_outliers(client_df, by_col=column)\n",
    "    f, ax = plot_churn_by(df, column, hist=hist, kde=kde, norm_hist=norm_hist)\n",
    "    plt.legend()\n",
    "    plt.close()\n",
    "    return f\n",
    "\n",
    "f, ax = plot_churn_by(client_df, 'age_years')\n",
    "ax = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the correlation matrix above, the two features that showed a negative correlation with churn were age and activity level. Here we generate a boxplot with those two features as the axes, and churn as the category. The plot shows that clients that churn tend to be younger across all levels of activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col = 'age_years'\n",
    "data = filter_outliers(client_df, by_col=col)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12,8))\n",
    "ax = sns.boxplot(x='activity_level', y=col, hue=\"churn\", data=data, \n",
    "                 palette='muted', ax=ax)\n",
    "title = ax.set_title('Client Churn by Activity Level')\n",
    "label = ax.set_ylabel('Age (Years)')\n",
    "label = ax.set_xlabel('Activity Level')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend = ax.legend(handles, churn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This beeswarm plot shows clients binned by the level of activity they maintain with the bank. Clients that churned maintained lower levels of activity (0-2). And of clients within these lower activity levels, younger clients churned more than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,8))\n",
    "ax = sns.swarmplot(x='activity_level', y='age_years', hue='churn', \n",
    "                   data=data.sample(n=100, random_state=51), \n",
    "                   palette='muted', ax=ax)\n",
    "title = ax.set_title('Client Churn by Activity Level')\n",
    "label = ax.set_ylabel('Age (Years)')\n",
    "label = ax.set_xlabel('Activity Level')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend = ax.legend(handles, churn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train churn model\n",
    "We now start to do some predictive analyses on the data to evaluate customer churn. To keep things simple, we use a single data set, which we split into training and test data sets. We use the training data to train the model, and the test data to make predictions about lost revenue to the bank.\n",
    "\n",
    "We use a supervised learning algorithm, random forest, to train the model. Random Forest is a popular algorithm for both classification and regression. It requires very little tuning and is less prone to overfitting. Random forest is an aggregation of decision trees where each tree classifies an observation in a dataset. Since random forest aggregates many classifiers, it is considered an ensemble method. Using scikit learn, we create our random forest for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_feature_space(df):\n",
    "    '''Create the feature space required by our classifier.'''\n",
    "    # drop columns/features we don't want/need for the classifier\n",
    "    features_df = df.drop(['churn', 'customer_id'], axis=1, errors='ignore')\n",
    "    X = features_df.as_matrix().astype(np.float)\n",
    "    # normalize feature values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def predict_churn(X):\n",
    "    '''Predict the probabilit of churn from feature set.'''\n",
    "    return clf.predict_proba(X)[:,1]\n",
    "\n",
    "def train_model(X, y):\n",
    "    '''Train our classifier using features X and target variable y.'''\n",
    "    clf = RF(n_estimators=100)\n",
    "    return clf.fit(X, y)\n",
    "\n",
    "def init_model(df):\n",
    "    # split data into train, test sets\n",
    "    train_index, test_index = train_test_split(df.index, random_state=99)\n",
    "    train_df = client_df.ix[train_index]\n",
    "    test_df = client_df.ix[test_index]\n",
    "\n",
    "    # target variable\n",
    "    y = np.array(train_df['churn'])\n",
    "\n",
    "    # extract features\n",
    "    X = make_feature_space(train_df)\n",
    "\n",
    "    # train classifier\n",
    "    clf = train_model(X, y)\n",
    "\n",
    "    return clf, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, the churn classifier and the test data set are used for our churn predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf, test_df = init_model(client_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate business loss\n",
    "In this simple example, we calculate the predicted loss of business (revenue) for all clients in the test data set. We calculate the revenue from each client, and multiply that by the churn probability to determine the predicted loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_business_loss(df):\n",
    "    df['customer_id'] = df.index\n",
    "    data = df.copy()\n",
    "\n",
    "    # extract features\n",
    "    X = make_feature_space(df)\n",
    "    \n",
    "    # predict churn\n",
    "    data['churn_probability'] = predict_churn(X)\n",
    "    \n",
    "    # TODO: avg_daily_balance would be a nice feature to have here\n",
    "    # for now, we'll just use fraction of income\n",
    "    avg_daily_balance = df['annual_income'] / 6\n",
    "\n",
    "    # Interest made on deposits\n",
    "    deposit_rate = 0.02\n",
    "\n",
    "    # Fee collected for each credit txn\n",
    "    credit_rate = 0.015\n",
    "\n",
    "    # Assume we make some money on trading fees and/or portfolio management\n",
    "    mgmt_rate = 0.02\n",
    "\n",
    "    # How much is each customer worth to the business?\n",
    "    worth = deposit_rate * avg_daily_balance + \\\n",
    "            mgmt_rate * df['annual_invest'] + \\\n",
    "            credit_rate * df['total_txn_amount']\n",
    "    data['worth'] = worth\n",
    "    \n",
    "    # How much would we lose per annum?\n",
    "    data['predicted_loss'] = data['churn_probability'] * worth\n",
    "    \n",
    "    return data.sort_values(by='predicted_loss', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churn_df = calc_business_loss(test_df)\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss by Age Group\n",
    "In this section, we calculate and plot the predicted loss of revenue by age group. In our data set, age is an important feature in predicting if a client will churn. We create a DataFrame containing the cumulative predicted loss by age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def group_by_age(df, bins=None):\n",
    "    if bins is None:\n",
    "        bin_size = 5\n",
    "        _min, _max = int(df.age_years.min()), int(df.age_years.max())\n",
    "        bins = range(_min, _max + bin_size, 5)\n",
    "    return df.groupby(pd.cut(df.age_years, bins=bins))\n",
    "\n",
    "data_by_age = churn_df.pipe(group_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_by_age_df = data_by_age['predicted_loss'].sum().reset_index()\n",
    "loss_by_age_df['age_years'] = loss_by_age_df['age_years'].astype(str)\n",
    "\n",
    "loss_by_age_df.plot(x='age_years', y='predicted_loss', style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
